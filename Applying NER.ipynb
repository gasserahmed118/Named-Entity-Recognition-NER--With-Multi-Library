{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d5395a0",
   "metadata": {},
   "source": [
    "# Imports & Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb029998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "install(\"spacy\")\n",
    "install(\"nltk\")\n",
    "install(\"transformers\")\n",
    "install(\"torch\")\n",
    "install(\"stanza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "948c1528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gasse\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 174MB/s]                     \n",
      "2025-12-28 18:38:50 INFO: Downloaded file to C:\\Users\\gasse\\stanza_resources\\resources.json\n",
      "2025-12-28 18:38:50 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-12-28 18:38:52 INFO: File exists: C:\\Users\\gasse\\stanza_resources\\en\\default.zip\n",
      "2025-12-28 18:38:55 INFO: Finished downloading models and saved to C:\\Users\\gasse\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "import stanza\n",
    "stanza.download(\"en\")\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358792ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\"])\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
    "nltk.download(\"maxent_ne_chunker\", quiet=True)\n",
    "nltk.download(\"maxent_ne_chunker_tab\", quiet=True)  # ðŸ”¥ THIS WAS MISSING\n",
    "nltk.download(\"words\", quiet=True)\n",
    "\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd43957f",
   "metadata": {},
   "source": [
    "# SpaCy NER â€“ Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "200243e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_spacy = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed922a3",
   "metadata": {},
   "source": [
    "# SpaCy Basic NER Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80105f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple | to | build | a | Hong | Kong | factory | for | $ | 6 | million | \n",
      "\n",
      "Entities:\n",
      "Apple ORG Companies, agencies, institutions, etc.\n",
      "0 1 0 5\n",
      "----------------------------------------\n",
      "Hong Kong GPE Countries, cities, states\n",
      "4 6 17 26\n",
      "----------------------------------------\n",
      "$6 million MONEY Monetary values, including unit\n",
      "8 11 39 49\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"Apple to build a Hong Kong factory for $6 million\"\n",
    "doc = nlp_spacy(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, end=\" | \")\n",
    "\n",
    "print(\"\\n\\nEntities:\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_, spacy.explain(ent.label_))\n",
    "    print(ent.start, ent.end, ent.start_char, ent.end_char)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913c90a0",
   "metadata": {},
   "source": [
    "# SpaCy Entity Visualization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c43ea73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Washington DC - GPE - Countries, cities, states\n",
      "------------------------------\n",
      "next May - DATE - Absolute or relative dates or periods\n",
      "------------------------------\n",
      "500 dollars - MONEY - Monetary values, including unit\n",
      "------------------------------\n",
      "Microsoft - ORG - Companies, agencies, institutions, etc.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def show_ents_spacy(doc):\n",
    "    if not doc.ents:\n",
    "        print(\"No named entities found.\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"{ent.text} - {ent.label_} - {spacy.explain(ent.label_)}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "show_ents_spacy(nlp_spacy(\"May I go to Washington DC next May?\"))\n",
    "show_ents_spacy(nlp_spacy(\"Can I borrow 500 dollars from Microsoft?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a24f47",
   "metadata": {},
   "source": [
    "# Adding Custom Entity In SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0683eb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPRO - ORG - Companies, agencies, institutions, etc.\n",
      "------------------------------\n",
      "U.K. - GPE - Countries, cities, states\n",
      "------------------------------\n",
      "$6 million - MONEY - Monetary values, including unit\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_spacy(\"CPRO to build a U.K. factory for $6 million\")\n",
    "ORG = doc.vocab.strings[\"ORG\"]\n",
    "new_ent = Span(doc, 0, 1, label=ORG)\n",
    "doc.ents = list(doc.ents) + [new_ent]\n",
    "\n",
    "show_ents_spacy(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d821ebe",
   "metadata": {},
   "source": [
    "# SpaCy Noun Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd50e800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars cars nsubj nominal subject\n",
      "----------------------------------------\n",
      "insurance liability liability dobj direct object\n",
      "----------------------------------------\n",
      "manufacturers manufacturers pobj object of preposition\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_spacy(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_, spacy.explain(chunk.root.dep_))\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21503fea",
   "metadata": {},
   "source": [
    "# SpaCy Large Text NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc00175e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No named entities found.\n"
     ]
    }
   ],
   "source": [
    "long_doc = nlp_spacy(\"Inflation affects economies in various positive and negative ways.\")\n",
    "show_ents_spacy(long_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7f52d7",
   "metadata": {},
   "source": [
    "# SpaCy Arabic Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6126de15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ø°Ù‡Ø¨ Ù…Ø­Ù…Ø¯ - ORG - Companies, agencies, institutions, etc.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "doc_ar = nlp_spacy(\"Ø°Ù‡Ø¨ Ù…Ø­Ù…Ø¯ Ø¥Ù„Ù‰ Ù…ØµØ± Ùˆ Ù„Ø¨Ù†Ø§Ù†\")\n",
    "show_ents_spacy(doc_ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a27a43",
   "metadata": {},
   "source": [
    "# NLTK NER (POS + Chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00ec41e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Mohamed/NNP)\n",
      "  (PERSON Salah/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Hawaii/NNP))\n"
     ]
    }
   ],
   "source": [
    "def nltk_ner(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    tree = ne_chunk(pos_tags)\n",
    "    return tree\n",
    "\n",
    "print(nltk_ner(\"Mohamed Salah was born in Hawaii\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2520d13",
   "metadata": {},
   "source": [
    "# HUGGING FACE TRANSFORMERS NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ef7d317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gasse\\AppData\\Local\\Python\\pythoncore-3.10-64\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gasse\\.cache\\huggingface\\hub\\models--dslim--bert-base-NER. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG 0.9987307\n",
      "U. K. LOC 0.99434024\n"
     ]
    }
   ],
   "source": [
    "ner_hf = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"dslim/bert-base-NER\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "hf_result = ner_hf(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in hf_result:\n",
    "    print(ent[\"word\"], ent[\"entity_group\"], ent[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c711821",
   "metadata": {},
   "source": [
    "# STANFORD NER (via Stanza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1802946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-28 18:59:40 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 10.1MB/s]                    \n",
      "2025-12-28 18:59:40 INFO: Downloaded file to C:\\Users\\gasse\\stanza_resources\\resources.json\n",
      "2025-12-28 18:59:40 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-12-28 18:59:41 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2025-12-28 18:59:41 INFO: Using device: cpu\n",
      "2025-12-28 18:59:41 INFO: Loading: tokenize\n",
      "2025-12-28 18:59:41 INFO: Loading: mwt\n",
      "2025-12-28 18:59:41 INFO: Loading: ner\n",
      "2025-12-28 18:59:45 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "California GPE\n",
      "Larry Page PERSON\n"
     ]
    }
   ],
   "source": [
    "nlp_stanza = stanza.Pipeline(\"en\", processors=\"tokenize,ner\")\n",
    "\n",
    "doc = nlp_stanza(\"Google was founded in California by Larry Page\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e0818",
   "metadata": {},
   "source": [
    "# Unified NER Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3384380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_ner(text):\n",
    "    print(\"spaCy:\")\n",
    "    show_ents_spacy(nlp_spacy(text))\n",
    "\n",
    "    print(\"\\nNLTK:\")\n",
    "    print(nltk_ner(text))\n",
    "\n",
    "    print(\"\\nHugging Face:\")\n",
    "    for ent in ner_hf(text):\n",
    "        print(ent[\"word\"], ent[\"entity_group\"])\n",
    "\n",
    "    print(\"\\nStanford (Stanza):\")\n",
    "    doc = nlp_stanza(text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa66b8",
   "metadata": {},
   "source": [
    "# Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cb8905c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy:\n",
      "Microsoft - ORG - Companies, agencies, institutions, etc.\n",
      "------------------------------\n",
      "Egypt - GPE - Countries, cities, states\n",
      "------------------------------\n",
      "5 million dollars - MONEY - Monetary values, including unit\n",
      "------------------------------\n",
      "\n",
      "NLTK:\n",
      "(S\n",
      "  (PERSON Microsoft/NNP)\n",
      "  acquired/VBD\n",
      "  a/DT\n",
      "  company/NN\n",
      "  in/IN\n",
      "  (GPE Egypt/NNP)\n",
      "  for/IN\n",
      "  5/CD\n",
      "  million/CD\n",
      "  dollars/NNS)\n",
      "\n",
      "Hugging Face:\n",
      "Microsoft ORG\n",
      "Egypt LOC\n",
      "\n",
      "Stanford (Stanza):\n",
      "Microsoft ORG\n",
      "Egypt GPE\n",
      "5 million dollars MONEY\n"
     ]
    }
   ],
   "source": [
    "run_all_ner(\"Microsoft acquired a company in Egypt for 5 million dollars\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
